# llm_gpu_deployment
Deploy LLM model from HuggingFace on GPU-instance in AWS as Docker API, test running performance, compare Docker API vs HuggingFace API
